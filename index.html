<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Guide: Processing Electronics Dataset</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <style>
        .code-block {
            background-color: #f6f8fa;
            border-radius: 8px;
            overflow-x: auto;
        }

        .code-block pre {
            padding: 1.5rem;
            margin: 0;
        }

        .section-card {
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .section-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
        }

        .highlight-link {
            color: #3b82f6;
            text-decoration: underline;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .highlight-link:hover {
            color: #1d4ed8;
        }
    </style>
</head>

<body class="bg-gradient-to-br from-gray-100 to-gray-200 font-sans text-gray-800 antialiased min-h-screen">
    <div class="container mx-auto px-4 py-12 max-w-5xl">
        <header class="text-center mb-12">
            <h1 class="text-5xl font-extrabold text-gray-900 mb-4 drop-shadow-md">Guide: Processing Electronics Dataset
            </h1>
            <p class="text-xl text-gray-700">Scalable Cloud-Based Sentiment Analysis for Amazon Electronics Reviews</p>
        </header>

        <main class="space-y-8">
            <section class="bg-white rounded-xl shadow-lg p-8 section-card" aria-labelledby="phase1-data-prep">
                <h2 id="phase1-data-prep"
                    class="text-3xl font-semibold text-gray-800 mb-6 border-b-2 border-indigo-500 pb-2">Phase 1: Data
                    Preparation</h2>
                <p class="text-lg text-gray-600 leading-relaxed">Upload the original dataset, Electronics.json (around
                    11 GB) to S3.</p>
            </section>

            <section class="bg-white rounded-xl shadow-lg p-8 section-card" aria-labelledby="phase1-emr">
                <h2 id="phase1-emr" class="text-3xl font-semibold text-gray-800 mb-6 border-b-2 border-indigo-500 pb-2">
                    Phase 1: AWS EMR Setup and Processing</h2>
                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">Create EMR Cluster</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Create a cluster on EMR with 2 cores. Use the
                    following bootstrap script:</p>
                <div class="code-block">
                    <pre><code class="language-bash">
#!/bin/bash
                    
# Exit on any error
set -e
                    
# Log file for debugging
LOG_FILE="/tmp/bootstrap.log"
echo "Starting bootstrap script..." | tee -a $LOG_FILE
                    
# Update system packages
echo "Updating system packages..." | tee -a $LOG_FILE
sudo yum update -y >> $LOG_FILE 2>&1
                    
# Check Python version
PYTHON_VERSION=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)
NLTK_DATA_PATH="/usr/local/lib/python${PYTHON_VERSION}/dist-packages/nltk_data"
echo "Detected Python version: $PYTHON_VERSION" | tee -a $LOG_FILE
echo "NLTK data path: $NLTK_DATA_PATH" | tee -a $LOG_FILE
                    
# Install Python dependencies
echo "Installing Python dependencies..." | tee -a $LOG_FILE
sudo pip3 install boto3 pandas plotly kaleido textblob numpy nltk >> $LOG_FILE 2>&1
                    
# Install NLTK data for TextBlob
echo "Installing NLTK data..." | tee -a $LOG_FILE
sudo mkdir -p $NLTK_DATA_PATH
sudo python3 -c "import nltk; nltk.download('punkt', download_dir='$NLTK_DATA_PATH');
nltk.download('averaged_perceptron_tagger', download_dir='$NLTK_DATA_PATH'); nltk.download('brown',
download_dir='$NLTK_DATA_PATH')" >> $LOG_FILE 2>&1
                    
# Install Google Chrome for Kaleido (Plotly image rendering)
echo "Installing Google Chrome..." | tee -a $LOG_FILE
sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm >> $LOG_FILE 2>&1
sudo yum install -y ./google-chrome-stable_current_x86_64.rpm >> $LOG_FILE 2>&1
sudo rm -f google-chrome-stable_current_x86_64.rpm >> $LOG_FILE 2>&1
                    
# Verify installations
echo "Verifying Python package installations..." | tee -a $LOG_FILE
pip3 show boto3 pandas plotly kaleido textblob numpy nltk >> $LOG_FILE 2>&1
                    
# Check NLTK data
echo "Verifying NLTK data..." | tee -a $LOG_FILE
ls -l $NLTK_DATA_PATH >> $LOG_FILE 2>&1
                    
echo "Bootstrap script completed successfully." | tee -a $LOG_FILE
exit 0
                    </code></pre>
                </div>
                <p class="mt-6 text-lg text-gray-600 leading-relaxed">Write the script on your local machine via Visual
                    Studio Code, save it, upload to S3, and use it as the bootstrap file when creating the cluster.</p>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">EMR Processing Script</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run the following PySpark script (<a
                        class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/data_ingestion.py">data_ingestion.py</a>)
                    on the EMR cluster:</p>
                <div class="code-block">
                    <pre><code class="language-python">
import logging
major_version = spark.version.split('.')[0]
if major_version == '3':
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, lower, spark_partition_id
    from pyspark.sql.types import StructType, StructField, StringType
else:
    from pyspark.sql import SQLContext
    from pyspark.sql.functions import col, lower, spark_partition_id
    from pyspark.sql.types import StructType, StructField, StringType
    sc = SparkContext.getOrCreate()
    spark = SQLContext(sc)
import logging
import boto3
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, spark_partition_id
from pyspark.sql.types import StructType, StructField, StringType

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

INPUT_PATH = "s3://electronics-reviews-bucket/Electronics.json"
OUTPUT_PATH = "s3://electronics-reviews-bucket/processed_text_data_original/"
BUCKET_NAME = "electronics-reviews-bucket"
OUTPUT_PREFIX = "processed_text_data_original/"

def main():
    spark = SparkSession.builder.appName("ElectronicsTextDataIngestion").getOrCreate()
    schema = StructType([StructField("reviewText", StringType(), True)])
    try:
        df_text = spark.read.option("multiLine", False).schema(schema).json(INPUT_PATH) \
            .filter(col("reviewText").isNotNull()) \
            .select(lower(col("reviewText")).alias("cleaned_reviewText"))
        df_text.write.mode("overwrite").parquet(OUTPUT_PATH)
        record_count = df_text.count()
        partition_count = df_text.select(spark_partition_id()).distinct().count()
        logger.info(f"Total records ingested: {record_count}")
        logger.info(f"Number of partitions: {partition_count}")
        s3_client = boto3.client('s3')
        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=OUTPUT_PREFIX)
        total_size_bytes = sum(obj['Size'] for obj in response.get('Contents', []))
        total_size_gb = total_size_bytes / (1024 ** 3)
        logger.info(f"Total output file size: {total_size_gb:.2f} GB")
    except Exception as e:
        logger.error(f"Error during data ingestion: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
                    </code></pre>
                </div>
            </section>

            <section class="bg-white rounded-xl shadow-lg p-8 section-card" aria-labelledby="phase2">
                <h2 id="phase2" class="text-3xl font-semibold text-gray-800 mb-6 border-b-2 border-indigo-500 pb-2">
                    Phase 2: MapReduce Implementation</h2>
                <p class="text-lg text-gray-600 mb-6 leading-relaxed">Add the following files to the project:</p>
                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">20percent.py</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run this script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/20percent.py">20percent.py</a>)
                    for 20% data processing:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">40percent.py</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run this script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/40percent.py">40percent.py</a>)
                    for 40% data processing:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">60percent.py</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run this script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/60percent.py">60percent.py</a>)
                    for 60% data processing:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">80percent.py</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run this script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/80percent.py">80percent.py</a>)
                    for 80% data processing:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">100percent.py</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">Run this script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/100percent.py">100percent.py</a>)
                    for 100% data processing:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>
            </section>

            <section class="bg-white rounded-xl shadow-lg p-8 section-card" aria-labelledby="phase2-stream">
                <h2 id="phase2-stream"
                    class="text-3xl font-semibold text-gray-800 mb-6 border-b-2 border-indigo-500 pb-2">
                    Phase 2: Stream Processing</h2>
                <p class="text-lg text-gray-600 mb-6 leading-relaxed">After completing the initial data processing, the
                    dataset was stored in Parquet format. However, streaming Parquet files directly using Kinesis and
                    PySpark proved challenging due to numerous errors related to Java dependencies and compatibility
                    issues. To overcome this, the Parquet files were converted to JSON format first.</p>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">Convert Parquet to JSON</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">A t3.medium EC2 instance was launched to execute
                    the following script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/convert_parquet_to_json.py">convert_parquet_to_json.py</a>)
                    to convert the processed Parquet files into JSON and save them to an S3 bucket:</p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">Send JSON to Kinesis</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">An Amazon Kinesis Data Stream was created.
                    <strong>⚠️ Ensure file names and file paths in the code are correct before proceeding.</strong> The
                    following script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/send_json_to_kinesis.py">send_json_to_kinesis.py</a>)
                    was saved on the EC2 instance and used to send the JSON data to Kinesis:
                </p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>

                <h3 class="text-2xl font-medium text-gray-700 mb-4 mt-6">Stream Processing</h3>
                <p class="text-lg text-gray-600 mb-4 leading-relaxed">The following script (<a class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/stream_processing.py">stream_processing.py</a>)
                    was used for real-time stream processing. To run the process smoothly, two separate terminal
                    sessions were opened on the EC2 instance via SSH. In one terminal,
                    <code>send_json_to_kinesis.py</code> was executed to send the data. In the other terminal,
                    <code>stream_processing.py</code> was run to process the stream in real time:
                </p>
                <div class="code-block">
                    <pre><code class="language-python">
<Code Here>
                    </code></pre>
                </div>
                <p class="text-lg text-gray-600 mt-4 leading-relaxed">With these steps completed, the stream processing
                    phase was successfully executed.</p>
            </section>

            <section class="bg-white rounded-xl shadow-lg p-8 section-card" aria-labelledby="resources">
                <h2 id="resources" class="text-3xl font-semibold text-gray-800 mb-6 border-b-2 border-indigo-500 pb-2">
                    Additional Resources</h2>
                <p class="text-lg text-gray-600 leading-relaxed">For more details, refer to the sample report (<a
                        class="highlight-link"
                        href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/Scalable_Cloud_Computing_Report.pdf">Scalable_Cloud_Computing_Report.pdf</a>).
                </p>
            </section>
        </main>

        <footer class="text-center mt-12 py-6 bg-indigo-900 text-white rounded-xl shadow-lg">
            <p class="text-lg">© 2025 Shwe Moe Thant, National College of Ireland</p>
            <p class="text-sm mt-2">Last Updated: July 03, 2025</p>
        </footer>
    </div>
</body>

</html>
