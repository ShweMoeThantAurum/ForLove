<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Guide: Processing Electronics Dataset</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>

<body class="bg-gray-50 font-sans antialiased">
    <div class="container mx-auto px-4 py-8 max-w-4xl">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-gray-800 mb-2">Guide: Processing Electronics Dataset</h1>
            <p class="text-lg text-gray-600">Scalable Cloud-Based Sentiment Analysis for Amazon Electronics Reviews</p>
        </header>

        <main>
            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase1-data-prep">
                <h2 id="phase1-data-prep" class="text-2xl font-semibold text-gray-700 mb-4">Phase 1: Data Preparation</h2>
                <p class="mb-4 text-gray-600">Upload the original dataset, Electronics.json (around 11 GB) to S3.</p>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase1-emr">
                <h2 id="phase1-emr" class="text-2xl font-semibold text-gray-700 mb-4">Phase 1: AWS EMR Setup and Processing</h2>
                <h3 class="text-xl font-medium text-gray-600 mb-2">Create EMR Cluster</h3>
                <p class="mb-4 text-gray-600">Create a cluster on EMR with 2 cores. Use the following bootstrap script:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-bash">
#!/bin/bash

# Exit on any error
set -e

# Update system packages
echo "Updating system packages..."
sudo yum update -y

# Install Python dependencies
echo "Installing Python dependencies..."
sudo pip3 install boto3 pandas plotly kaleido vaderSentiment numpy nltk

# Install NLTK data (if needed for other scripts like data_ingestion.py)
echo "Installing NLTK data..."
sudo python3 -c "import nltk; nltk.download('punkt', download_dir='/usr/local/lib/python3.8/dist-packages/nltk_data'); nltk.download('averaged_perceptron_tagger', download_dir='/usr/local/lib/python3.8/dist-packages/nltk_data')"

# Install Google Chrome for Kaleido (Plotly image rendering)
echo "Installing Google Chrome..."
sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
sudo yum install -y ./google-chrome-stable_current_x86_64.rpm
sudo rm -f google-chrome-stable_current_x86_64.rpm

# Verify installations
echo "Verifying Python package installations..."
pip3 show boto3 pandas plotly kaleido vaderSentiment numpy nltk

echo "Bootstrap script completed successfully."                </code></pre>
                <p class="mt-4 text-gray-600">Write the script on your local machine via Visual Studio Code, save it, upload to S3, and use it as the bootstrap file when creating the cluster.</p>

                <h3 class="text-xl font-medium text-gray-600 mb-2">EMR Processing Script</h3>
                <p class="mb-4 text-gray-600">Run the following PySpark script (<a href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/data_ingestion.py">data_ingestion.py</a>) on the EMR cluster:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-python">
import logging
import boto3
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, spark_partition_id
from pyspark.sql.types import StructType, StructField, StringType

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

INPUT_PATH = "s3://electronics-reviews-bucket/Electronics.json"
OUTPUT_PATH = "s3://electronics-reviews-bucket/processed_text_data_original/"
BUCKET_NAME = "electronics-reviews-bucket"
OUTPUT_PREFIX = "processed_text_data_original/"

def main():
    spark = SparkSession.builder.appName("ElectronicsTextDataIngestion").getOrCreate()
    schema = StructType([StructField("reviewText", StringType(), True)])
    try:
        df_text = spark.read.option("multiLine", False).schema(schema).json(INPUT_PATH) \
            .filter(col("reviewText").isNotNull()) \
            .select(lower(col("reviewText")).alias("cleaned_reviewText"))
        df_text.write.mode("overwrite").parquet(OUTPUT_PATH)
        record_count = df_text.count()
        partition_count = df_text.select(spark_partition_id()).distinct().count()
        logger.info(f"Total records ingested: {record_count}")
        logger.info(f"Number of partitions: {partition_count}")
        s3_client = boto3.client('s3')
        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=OUTPUT_PREFIX)
        total_size_bytes = sum(obj['Size'] for obj in response.get('Contents', []))
        total_size_gb = total_size_bytes / (1024 ** 3)
        logger.info(f"Total output file size: {total_size_gb:.2f} GB")
    except Exception as e:
        logger.error(f"Error during data ingestion: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
                </code></pre>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase2">
                <h2 id="phase2" class="text-2xl font-semibold text-gray-700 mb-4">Phase 2: MapReduce Implementation</h2>
                <p class="mb-4 text-gray-600">If using the same cluster, install these dependencies:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-bash">
sudo yum update -y
pip3 install boto3
pip3 install pandas
pip3 install poorly
pip3 install -U kaleido
sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
sudo yum install -y google-chrome-stable_current_x86_64.rpm
                </code></pre>
                <p class="mt-4 text-gray-600">For future clusters, reuse the bootstrap file and install these dependencies. Run the following PySpark script (<a href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/keyword_extraction.py">keyword_extraction.py</a>):</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-python">
import logging
import boto3
import json
import time
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, spark_partition_id
from pyspark.ml.feature import Tokenizer, StopWordsRemover

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

INPUT_PATH = "s3://electronics-reviews-bucket/processed_text_data/"
OUTPUT_PATH = "s3://electronics-reviews-bucket/keyword_results/"
BUCKET_NAME = "electronics-reviews-bucket"
OUTPUT_PREFIX = "keyword_results/"
METRICS_PATH = "results/metrics/"
PLOT_PATH = "results/plots/"
TOTAL_DATA_SIZE_GB = 3.33

def plot_metrics(metrics_list, output_path, bucket_name):
    df = pd.DataFrame(metrics_list)
    fig = make_subplots(rows=2, cols=1, subplot_titles=("Throughput vs. Data Size", "Latency vs. Data Size"), vertical_spacing=0.15)
    fig.add_trace(go.Scatter(x=df['data_size_gb'], y=df['throughput_records_per_second'], mode='lines+markers', name='Throughput', line=dict(color='royalblue', width=2), marker=dict(size=8)), row=1, col=1)
    fig.add_trace(go.Scatter(x=df['data_size_gb'], y=df['latency_per_record_ms'], mode='lines+markers', name='Latency', line=dict(color='crimson', width=2), marker=dict(size=8)), row=2, col=1)
    fig.update_layout(height=800, width=1000, showlegend=True, title="Performance Metrics Across Data Sizes", title_x=0.5, font=dict(size=14), plot_bgcolor='white', paper_bgcolor='white')
    fig.update_xaxes(title_text="Data Size (GB)", row=1, col=1, gridcolor='lightgray')
    fig.update_xaxes(title_text="Data Size (GB)", row=2, col=1, gridcolor='lightgray')
    fig.update_yaxes(title_text="Throughput (records/s)", row=1, col=1, gridcolor='lightgray')
    fig.update_yaxes(title_text="Latency (ms/record)", row=2, col=1, gridcolor='lightgray')
    local_plot_path = "/tmp/performance_plots.png"
    fig.write_image(local_plot_path, format="png", scale=2)
    local_html_path = "/tmp/performance_plots.html"
    fig.write_html(local_html_path)
    s3_client = boto3.client('s3')
    s3_client.upload_file(local_plot_path, bucket_name, f"{output_path}performance_plots.png")
    s3_client.upload_file(local_html_path, bucket_name, f"{output_path}performance_plots.html")
    logger.info(f"Performance plots saved to s3://{bucket_name}/{output_path}performance_plots.png")
    logger.info(f"Interactive HTML plot saved to s3://{bucket_name}/{output_path}performance_plots.html")

def plot_top_words(word_freq, data_size_gb, output_path, bucket_name):
    top_words_df = word_freq.limit(10).select("word", "frequency").toPandas()
    fig = go.Figure(data=[go.Bar(x=top_words_df['word'], y=top_words_df['frequency'], marker_color='teal', text=top_words_df['frequency'], textposition='auto')])
    fig.update_layout(height=600, width=800, title=f"Top 10 Words for Data Size {data_size_gb:.2f} GB", title_x=0.5, font=dict(size=14), xaxis_title="Word", yaxis_title="Frequency", plot_bgcolor='white', paper_bgcolor='white', xaxis=dict(tickangle=45, gridcolor='lightgray'), yaxis=dict(gridcolor='lightgray'))
    local_plot_path = f"/tmp/top_words_data_size={data_size_gb:.2f}GB.png"
    fig.write_image(local_plot_path, format="png", scale=2)
    local_html_path = f"/tmp/top_words_data_size={data_size_gb:.2f}GB.html"
    fig.write_html(local_html_path)
    s3_client = boto3.client('s3')
    s3_client.upload_file(local_plot_path, bucket_name, f"{output_path}top_words_data_size={data_size_gb:.2f}GB.png")
    s3_client.upload_file(local_html_path, bucket_name, f"{output_path}top_words_data_size={data_size_gb:.2f}GB.html")
    logger.info(f"Top 10 words plot saved to s3://{bucket_name}/{output_path}top_words_data_size={data_size_gb:.2f}GB.png")
    logger.info(f"Interactive top 10 words HTML saved to s3://{bucket_name}/{output_path}top_words_data_size={data_size_gb:.2f}GB.html")

def run_experiment(spark, fraction, input_path, output_path, bucket_name, output_prefix, total_size_gb=3.33):
    logger.info(f"Running experiment with data fraction {fraction:.2f} (~{fraction*total_size_gb:.2f} GB)")
    metrics = {"data_size_gb": fraction * total_size_gb}
    start_time = time.time()
    df = spark.read.parquet(input_path).sample(fraction=fraction).cache()
    metrics["total_records"] = df.count()
    metrics["read_time_seconds"] = time.time() - start_time
    logger.info(f"Read {metrics['total_records']} records in {metrics['read_time_seconds']:.2f} seconds")
    tokenize_start = time.time()
    tokenizer = Tokenizer(inputCol="cleaned_reviewText", outputCol="words")
    df = tokenizer.transform(df)
    metrics["tokenize_time_seconds"] = time.time() - tokenize_start
    stopwords_start = time.time()
    remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
    df = remover.transform(df)
    metrics["stopwords_time_seconds"] = time.time() - stopwords_start
    wordfreq_start = time.time()
    df_words = df.select(explode(col("filtered_words")).alias("word")).filter(col("word") != "")
    word_freq = df_words.groupBy("word").count().withColumnRenamed("count", "frequency").orderBy(col("frequency").desc())
    metrics["wordfreq_time_seconds"] = time.time() - wordfreq_start
    save_start = time.time()
    output_dir = f"{output_path}data_size={fraction*total_size_gb:.2f}GB/"
    word_freq.write.mode("overwrite").parquet(output_dir)
    metrics["save_time_seconds"] = time.time() - save_start
    top_words = word_freq.limit(5).select("word", "frequency")
    top_words.show(5, truncate=False)
    plot_top_words(word_freq, fraction * total_size_gb, PLOT_PATH, BUCKET_NAME)
    metrics["total_execution_time_seconds"] = time.time() - start_time
    metrics["throughput_records_per_second"] = metrics["total_records"] / metrics["total_execution_time_seconds"] if metrics["total_execution_time_seconds"] > 0 else 0
    metrics["latency_per_record_ms"] = (metrics["total_execution_time_seconds"] / metrics["total_records"] * 1000) if metrics["total_records"] > 0 else 0
    metrics["partition_count"] = df.select(spark_partition_id()).distinct().count()
    return metrics

def main():
    spark = SparkSession.builder.appName("KeywordExtraction").getOrCreate()
    try:
        fractions = [0.2, 0.4, 0.6, 0.8, 1.0]
        metrics_list = []
        for fraction in fractions:
            metrics = run_experiment(spark, fraction, INPUT_PATH, OUTPUT_PATH, BUCKET_NAME, OUTPUT_PREFIX, TOTAL_DATA_SIZE_GB)
            metrics_list.append(metrics)
        metrics_json_path = "/tmp/keyword_metrics.json"
        with open(metrics_json_path, "w") as f:
            json.dump(metrics_list, f, indent=4)
        s3_client = boto3.client('s3')
        s3_client.upload_file(metrics_json_path, BUCKET_NAME, f"{METRICS_PATH}keyword_metrics.json")
        logger.info(f"Metrics JSON saved to s3://{BUCKET_NAME}/{METRICS_PATH}keyword_metrics.json")
        plot_metrics(metrics_list, PLOT_PATH, BUCKET_NAME)
        spark_config = {
            "spark.app.name": spark.conf.get("spark.app.name"),
            "spark.master": spark.conf.get("spark.master"),
            "spark.executor.cores": spark.conf.get("spark.executor.cores", "default"),
            "spark.executor.memory": spark.conf.get("spark.executor.memory", "default"),
            "spark.driver.memory": spark.conf.get("spark.driver.memory", "default"),
            "spark.sql.shuffle.partitions": int(spark.conf.get("spark.sql.shuffle.partitions", "200"))
        }
        config_path = "/tmp/spark_config.json"
        with open(config_path, "w") as f:
            json.dump(spark_config, f, indent=4)
        s3_client.upload_file(config_path, BUCKET_NAME, f"{METRICS_PATH}spark_config.json")
        logger.info(f"Spark config saved to s3://{BUCKET_NAME}/{METRICS_PATH}spark_config.json")
    except Exception as e:
        logger.error(f"Error during processing: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
                </code></pre>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6" aria-labelledby="resources">
                <h2 id="resources" class="text-2xl font-semibold text-gray-700 mb-4">Additional Resources</h2>
                <p class="text-gray-600">For more details, refer to the sample report (<a href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/Scalable_Cloud_Computing_Report.pdf">Scalable_Cloud_Computing_Report.pdf</a>).</p>
            </section>
        </main>
    </div>
</body>

</html>
