<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Guide: Processing Electronics Dataset</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>

<body class="bg-gray-50 font-sans antialiased">
    <div class="container mx-auto px-4 py-8 max-w-4xl">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-gray-800 mb-2">Guide: Processing Electronics Dataset</h1>
            <p class="text-lg text-gray-600">Scalable Cloud-Based Sentiment Analysis for Amazon Electronics Reviews</p>
        </header>

        <main>
            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase1-data-prep">
                <h2 id="phase1-data-prep" class="text-2xl font-semibold text-gray-700 mb-4">Phase 1: Data Preparation</h2>
                <p class="mb-4 text-gray-600">Upload the original dataset, Electronics.json (around 11 GB) to S3.</p>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase1-emr">
                <h2 id="phase1-emr" class="text-2xl font-semibold text-gray-700 mb-4">Phase 1: AWS EMR Setup and Processing</h2>
                <h3 class="text-xl font-medium text-gray-600 mb-2">Create EMR Cluster</h3>
                <p class="mb-4 text-gray-600">Create a cluster on EMR with 2 cores. Use the following bootstrap script:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-bash">
#!/bin/bash
sudo pip3 install textblob numpy nltk
sudo python3 -c "import nltk; nltk.download('punkt', download_dir='/usr/local/lib/python3.8/dist-packages/nltk_data'); nltk.download('averaged_perceptron_tagger', download_dir='/usr/local/lib/python3.8/dist-packages/nltk_data')"
                </code></pre>
                <p class="mt-4 text-gray-600">Write the script on your local machine via Visual Studio Code, save it, upload to S3, and use it as the bootstrap file when creating the cluster.</p>

                <h3 class="text-xl font-medium text-gray-600 mb-2">EMR Processing Script</h3>
                <p class="mb-4 text-gray-600">Run the following PySpark script (<a href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/data_ingestion.py">data_ingestion.py</a>) on the EMR cluster:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-python">
import logging
import boto3
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, spark_partition_id
from pyspark.sql.types import StructType, StructField, StringType

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

INPUT_PATH = "s3://electronics-reviews-bucket/Electronics.json"
OUTPUT_PATH = "s3://electronics-reviews-bucket/processed_text_data/"
BUCKET_NAME = "electronics-reviews-bucket"
OUTPUT_PREFIX = "processed_text_data/"

def main():
    spark = SparkSession.builder.appName("ElectronicsTextDataIngestion").getOrCreate()
    schema = StructType([StructField("reviewText", StringType(), True)])
    try:
        df_text = spark.read.option("multiLine", False).schema(schema).json(INPUT_PATH) \
            .filter(col("reviewText").isNotNull()) \
            .select(lower(col("reviewText")).alias("cleaned_reviewText"))
        df_text.write.mode("overwrite").parquet(OUTPUT_PATH)
        record_count = df_text.count()
        partition_count = df_text.select(spark_partition_id()).distinct().count()
        logger.info(f"Total records ingested: {record_count}")
        logger.info(f"Number of partitions: {partition_count}")
        s3_client = boto3.client('s3')
        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=OUTPUT_PREFIX)
        total_size_bytes = sum(obj['Size'] for obj in response.get('Contents', []))
        total_size_gb = total_size_bytes / (1024 ** 3)
        logger.info(f"Total output file size: {total_size_gb:.2f} GB")
    except Exception as e:
        logger.error(f"Error during data ingestion: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
                </code></pre>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6 mb-6" aria-labelledby="phase2">
                <h2 id="phase2" class="text-2xl font-semibold text-gray-700 mb-4">Phase 2: MapReduce Implementation</h2>
                <p class="mb-4 text-gray-600">If using the same cluster, install these dependencies:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-bash">
sudo yum update -y
pip3 install boto3
pip3 install pandas
pip3 install poorly
pip3 install -U kaleido
sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
sudo yum install -y google-chrome-stable_current_x86_64.rpm
                </code></pre>
                <p class="mt-4 text-gray-600">For future clusters, reuse the bootstrap file and install these dependencies. Run the following combined PySpark script:</p>
                <pre class="bg-gray-100 p-4 rounded-lg overflow-x-auto"><code class="language-python">
import logging
import boto3
import json
import time
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, spark_partition_id, udf
from pyspark.sql.types import FloatType, StringType, StructType, StructField
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Constants
INPUT_PATH = "s3://electronics-reviews-bucket/processed_text_data/"
KEYWORD_OUTPUT_PATH = "s3://electronics-reviews-bucket/keyword_results/"
SENTIMENT_OUTPUT_PATH = "s3://electronics-reviews-bucket/sentiment_results/"
BUCKET_NAME = "electronics-reviews-bucket"
KEYWORD_PLOT_PATH = "results/keyword_plots/"
SENTIMENT_PLOT_PATH = "results/sentiment_plots/"
METRICS_PATH = "results/metrics/"
TOTAL_DATA_SIZE_GB = 3.33

# UDF for VADER sentiment analysis
analyzer = SentimentIntensityAnalyzer()
def get_sentiment(text):
    scores = analyzer.polarity_scores(text)
    compound = scores['compound']
    category = (
        'positive' if compound > 0.05 else
        'negative' if compound < -0.05 else
        'neutral'
    )
    return (compound, category)

sentiment_udf = udf(get_sentiment, StructType([
    StructField("compound", FloatType(), False),
    StructField("category", StringType(), False)
]))

def upload_to_s3(local_path, s3_key):
    s3_client = boto3.client('s3')
    s3_client.upload_file(local_path, BUCKET_NAME, s3_key)
    logger.info(f"Uploaded to s3://{BUCKET_NAME}/{s3_key}")

def plot_keyword_metrics(metrics_list):
    df = pd.DataFrame(metrics_list)
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=("Throughput vs. Data Size", "Latency vs. Data Size"),
        vertical_spacing=0.15
    )
    fig.add_trace(
        go.Scatter(
            x=df['data_size_gb'],
            y=df['throughput_records_per_second'],
            mode='lines+markers',
            name='Throughput',
            line=dict(color='royalblue', width=2),
            marker=dict(size=8)
        ),
        row=1, col=1
    )
    fig.add_trace(
        go.Scatter(
            x=df['data_size_gb'],
            y=df['latency_per_record_ms'],
            mode='lines+markers',
            name='Latency',
            line=dict(color='crimson', width=2),
            marker=dict(size=8)
        ),
        row=2, col=1
    )
    fig.update_layout(
        height=800,
        width=1000,
        showlegend=True,
        title="Keyword Extraction Performance Metrics",
        title_x=0.5,
        font=dict(size=14),
        plot_bgcolor='white',
        paper_bgcolor='white'
    )
    fig.update_xaxes(title_text="Data Size (GB)", gridcolor='lightgray')
    fig.update_yaxes(title_text="Throughput (records/s)", row=1, col=1, gridcolor='lightgray')
    fig.update_yaxes(title_text="Latency (ms/record)", row=2, col=1, gridcolor='lightgray')
    local_plot_path = "/tmp/keyword_performance.png"
    fig.write_image(local_plot_path, format="png", scale=2)
    upload_to_s3(local_plot_path, f"{KEYWORD_PLOT_PATH}performance.png")

def plot_sentiment_metrics(metrics_list):
    df = pd.DataFrame(metrics_list)
    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=("Throughput vs. Data Size", "Latency vs. Data Size", "Average Sentiment vs. Data Size"),
        vertical_spacing=0.15
    )
    fig.add_trace(go.Scatter(x=df['data_size_gb'], y=df['throughput_records_per_second'], mode='lines+markers', name='Throughput', line=dict(color='royalblue')), row=1, col=1)
    fig.add_trace(go.Scatter(x=df['data_size_gb'], y=df['latency_per_record_ms'], mode='lines+markers', name='Latency', line=dict(color='crimson')), row=2, col=1)
    fig.add_trace(go.Scatter(x=df['data_size_gb'], y=df['avg_sentiment'], mode='lines+markers', name='Avg Sentiment', line=dict(color='green')), row=3, col=1)
    fig.update_layout(height=1000, width=1000, title="Sentiment Analysis Performance Metrics", title_x=0.5, font=dict(size=14), plot_bgcolor='white')
    fig.update_xaxes(title_text="Data Size (GB)", gridcolor='lightgray')
    fig.update_yaxes(gridcolor='lightgray')
    png_path = "/tmp/sentiment_metrics.png"
    fig.write_image(png_path, scale=2)
    upload_to_s3(png_path, f"{SENTIMENT_PLOT_PATH}metrics.png")

def plot_top_words(word_freq, data_size_gb):
    top_words_df = word_freq.limit(10).select("word", "frequency").toPandas()
    fig = go.Figure(
        data=[
            go.Bar(
                x=top_words_df['word'],
                y=top_words_df['frequency'],
                marker_color='teal',
                text=top_words_df['frequency'],
                textposition='auto'
            )
        ]
    )
    fig.update_layout(
        height=600,
        width=800,
        title=f"Top 10 Words for Data Size {data_size_gb:.2f} GB",
        title_x=0.5,
        font=dict(size=14),
        xaxis_title="Word",
        yaxis_title="Frequency",
        plot_bgcolor='white',
        paper_bgcolor='white',
        xaxis=dict(tickangle=45, gridcolor='lightgray'),
        yaxis=dict(gridcolor='lightgray')
    )
    local_plot_path = f"/tmp/top_words_{data_size_gb:.2f}GB.png"
    fig.write_image(local_plot_path, format="png", scale=2)
    upload_to_s3(local_plot_path, f"{KEYWORD_PLOT_PATH}top_words_{data_size_gb:.2f}GB.png")

def plot_sentiment_distribution(sentiment_df, data_size_gb):
    df = sentiment_df.select("compound").toPandas()
    fig = go.Figure([go.Histogram(x=df['compound'], nbinsx=50, marker_color='purple')])
    fig.update_layout(title=f"Sentiment Distribution for {data_size_gb:.2f} GB", xaxis_title="Compound Score", yaxis_title="Count", title_x=0.5)
    png_path = f"/tmp/distribution_{data_size_gb:.2f}GB.png"
    fig.write_image(png_path, scale=2)
    upload_to_s3(png_path, f"{SENTIMENT_PLOT_PATH}distribution_{data_size_gb:.2f}GB.png")

def plot_sentiment_categories(category_counts, data_size_gb):
    df = pd.DataFrame(category_counts, columns=['category', 'count'])
    fig = go.Figure([go.Bar(x=df['category'], y=df['count'], text=df['count'], textposition='auto', marker_color=['green', 'red', 'blue'])])
    fig.update_layout(title=f"Sentiment Categories for {data_size_gb:.2f} GB", xaxis_title="Category", yaxis_title="Count", title_x=0.5)
    png_path = f"/tmp/categories_{data_size_gb:.2f}GB.png"
    fig.write_image(png_path, scale=2)
    upload_to_s3(png_path, f"{SENTIMENT_PLOT_PATH}categories_{data_size_gb:.2f}GB.png")

def run_experiment(spark, fraction):
    data_size_gb = fraction * TOTAL_DATA_SIZE_GB
    logger.info(f"Running with {fraction:.2%} of data (~{data_size_gb:.2f} GB)")
    metrics = {"data_size_gb": data_size_gb}

    # Read and cache data
    start = time.time()
    df = spark.read.parquet(INPUT_PATH).sample(fraction).cache()
    metrics["total_records"] = df.count()
    metrics["read_time_seconds"] = time.time() - start

    # Keyword extraction
    keyword_start = time.time()
    tokenizer = Tokenizer(inputCol="cleaned_reviewText", outputCol="words")
    df_words = tokenizer.transform(df)
    remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
    df_filtered = remover.transform(df_words)
    word_freq = df_filtered.select(explode(col("filtered_words")).alias("word")).filter(col("word") != "").groupBy("word").count().withColumnRenamed("count", "frequency").orderBy(col("frequency").desc())
    metrics["keyword_time_seconds"] = time.time() - keyword_start
    output_dir = f"{KEYWORD_OUTPUT_PATH}data_size={data_size_gb:.2f}GB/"
    word_freq.write.mode("overwrite").parquet(output_dir)
    metrics["keyword_save_time_seconds"] = time.time() - keyword_start
    plot_top_words(word_freq, data_size_gb)

    # Sentiment analysis
    sentiment_start = time.time()
    sentiment_df = df.select(
        "cleaned_reviewText",
        sentiment_udf(col("cleaned_reviewText")).alias("sentiment")
    ).select(
        "cleaned_reviewText",
        col("sentiment.compound").alias("compound"),
        col("sentiment.category").alias("category")
    )
    metrics["sentiment_time_seconds"] = time.time() - sentiment_start
    avg_sentiment = sentiment_df.agg({"compound": "avg"}).collect()[0][0]
    metrics["avg_sentiment"] = avg_sentiment
    category_counts_df = sentiment_df.groupBy("category").count()
    category_counts = [(row['category'], row['count']) for row in category_counts_df.collect()]
    for cat, count in category_counts:
        metrics[f"{cat}_count"] = count
    output_dir = f"{SENTIMENT_OUTPUT_PATH}data_size={data_size_gb:.2f}GB/"
    sentiment_df.write.mode("overwrite").parquet(output_dir)
    metrics["sentiment_save_time_seconds"] = time.time() - sentiment_start
    plot_sentiment_distribution(sentiment_df, data_size_gb)
    plot_sentiment_categories(category_counts, data_size_gb)

    # Metrics calculations
    metrics["total_execution_time_seconds"] = time.time() - start
    metrics["throughput_records_per_second"] = metrics["total_records"] / metrics["total_execution_time_seconds"] if metrics["total_execution_time_seconds"] > 0 else 0
    metrics["latency_per_record_ms"] = (metrics["total_execution_time_seconds"] / metrics["total_records"] * 1000) if metrics["total_records"] > 0 else 0
    metrics["partition_count"] = df.select(spark_partition_id()).distinct().count()
    return metrics

def main():
    spark = SparkSession.builder.appName("CombinedAnalysis").getOrCreate()
    try:
        fractions = [0.1, 0.25, 0.5, 0.75, 1.0]
        metrics_list = []
        for fraction in fractions:
            metrics = run_experiment(spark, fraction)
            metrics_list.append(metrics)
        metrics_json_path = "/tmp/combined_metrics.json"
        with open(metrics_json_path, "w") as f:
            json.dump(metrics_list, f, indent=4)
        upload_to_s3(metrics_json_path, f"{METRICS_PATH}combined_metrics.json")
        plot_keyword_metrics(metrics_list)
        plot_sentiment_metrics(metrics_list)
    except Exception as e:
        logger.error(f"Error during processing: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
                </code></pre>
            </section>

            <section class="bg-white rounded-lg shadow-md p-6" aria-labelledby="resources">
                <h2 id="resources" class="text-2xl font-semibold text-gray-700 mb-4">Additional Resources</h2>
                <p class="text-gray-600">For more details, refer to the sample report (<a href="https://github.com/ShweMoeThantAurum/Scalable-Cloud-Programming-Project/blob/main/Scalable_Cloud_Computing_Report.pdf">Scalable_Cloud_Computing_Report.pdf</a>).</p>
            </section>
        </main>

        <footer class="text-center mt-8 py-4 bg-gray-800 text-white rounded-lg">
            <p>© 2025 Shwe Moe Thant, National College of Ireland</p>
        </footer>
    </div>
</body>

</html>
